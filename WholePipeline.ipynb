{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import models, corpora\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEPS**\n",
    "\n",
    "1) Removing links, emojies, punctuations, numbers and missing values, making them lowercase\n",
    "\n",
    "2) Language detection\n",
    "\n",
    "3) Tokenization\n",
    "\n",
    "4) Removal of Stop Words (finding libraries for all languages)\n",
    "\n",
    "5) Lemmatization\n",
    "\n",
    "-----    Preprocessing is over\n",
    "\n",
    "6) LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"a_chat.csv\")\n",
    "my_list = data['message'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing links, emojies, punctuations, numbers and missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = []\n",
    "for x in my_list:\n",
    "    x = re.split('https:\\/\\/.*', str(x))[0] # remove links\n",
    "    x = ' '.join(re.sub(\"[0-9.,!?:;\\-='\\\"@#_]\", \" \", emoji.demojize(x)).split()) # remove punctuation, numbers, and emojies\n",
    "    if x != 'nan': # check if x is not nan\n",
    "        cleaned_list.append(x.lower()) # lowercase and append to cleaned_list\n",
    "cleaned_list = [x for x in cleaned_list if len(x) > 1] # remove single characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lang Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language of messages is en\n"
     ]
    }
   ],
   "source": [
    "#lang detection of list (if it is more than 50% a language, detect language as that one)\n",
    "from collections import Counter\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "# detect language of each message\n",
    "lang_list = [detect_lang(x) for x in cleaned_list/100]\n",
    "# count number of messages in each language\n",
    "lang_count = Counter(lang_list)\n",
    "# get most common language\n",
    "most_common_lang = lang_count.most_common(1)[0][0]\n",
    "# calculate percentage of most common language\n",
    "most_common_lang_percentage = lang_count[most_common_lang] / len(cleaned_list)\n",
    "# if most common language is more than 50% of messages, say that language is the language of the messages\n",
    "if most_common_lang_percentage > 0.5:\n",
    "    print(\"Language of messages is\", most_common_lang)\n",
    "else:\n",
    "    print(\"No dominant language detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize each string in the list\n",
    "tokenized_list = [word_tokenize(x) for x in cleaned_list]\n",
    "tokenized_list = [x for x in tokenized_list if len(x) > 1] # remove single characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang_codes = pd.read_csv(\"lang_codes.csv\")\n",
    "lang_codes[\"stop_words\"] = lang_codes[\"stop_words\"].apply(lambda x: ast.literal_eval(x) if type(x) == str else x)\n",
    "stop_words_list = lang_codes[lang_codes[\"languages\"] == \"en\"][\"stop_words\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_list)):\n",
    "    tokenized_list[i] = [x for x in tokenized_list[i] if x not in stop_words_list] # remove stopwords\n",
    "    tokenized_list[i] = [x for x in tokenized_list[i] if len(x) > 1] # remove single characters\n",
    "    tokenized_list[i] = [x for x in tokenized_list[i] if x.isalpha()] # remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Calculate the number of sublists you want to select (1% of the total)\n",
    "num_sublists_to_select = len(tokenized_list) \n",
    "# Use random.sample to select sublists randomly without duplicates\n",
    "selected_sublists = random.sample(tokenized_list, num_sublists_to_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\"+\"_core_web_sm\")\n",
    "lemmatized_list = [[token.lemma_ for token in nlp(\" \".join(doc))] for doc in selected_sublists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA TOPIC MODELING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(lemmatized_list)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in lemmatized_list]\n",
    "\n",
    "# Create the LDA model\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,  # Define the number of topics you want to extract\n",
    "    passes= 10    # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "topics = lda_model.print_topics(num_words=20)\n",
    "\n",
    "for topic in topics:\n",
    "    words = topic[1].split(\" + \")\n",
    "    print(\"Topic {}: {}\".format(topic[0], \", \".join(word.split(\"*\")[1] for word in words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/suhedayildirim/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment for each document\n",
    "sentiments = []\n",
    "\n",
    "for doc in lemmatized_list:\n",
    "    sentiment = sid.polarity_scores(\" \".join(doc))\n",
    "    sentiments.append(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store sentiments for each topic\n",
    "topic_sentiments = {i: [] for i in range(20)}  # Replace 20 with your actual number of topics\n",
    "\n",
    "# Associate documents with topics based on the LDA model\n",
    "topic_assignments = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in doc_term_matrix]\n",
    "\n",
    "# Group documents by topic and aggregate sentiments\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    topic = topic_assignments[i]\n",
    "    topic_sentiments[topic].append(sentiment)\n",
    "\n",
    "# Calculate average sentiment scores for each topic\n",
    "topic_average_sentiments = {}\n",
    "for topic, topic_sent_list in topic_sentiments.items():\n",
    "    if topic_sent_list:\n",
    "        average_sentiment = {\n",
    "            'compound': sum(s['compound'] for s in topic_sent_list) / len(topic_sent_list),\n",
    "            'pos': sum(s['pos'] for s in topic_sent_list) / len(topic_sent_list),\n",
    "            'neg': sum(s['neg'] for s in topic_sent_list) / len(topic_sent_list),\n",
    "            'neu': sum(s['neu'] for s in topic_sent_list) / len(topic_sent_list)\n",
    "        }\n",
    "        topic_average_sentiments[topic] = average_sentiment\n",
    "\n",
    "# Print average sentiment scores for each topic\n",
    "for topic, sentiment in topic_average_sentiments.items():\n",
    "    print(f\"Topic {topic}: Compound: {sentiment['compound']}, Pos: {sentiment['pos']}, Neg: {sentiment['neg']}, Neu: {sentiment['neu']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
